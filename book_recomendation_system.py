# -*- coding: utf-8 -*-
"""Book_recomendation_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yShJBFCrs5bMhvHBmBMgBQL_eN8MYeaA

# **BOOK RECOMENDATION SYSTEM**

## Import Library
"""

# Import library
import kagglehub
import os
import pandas as pd
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""## Load Dataset"""

path = kagglehub.dataset_download("ruchi798/bookcrossing-dataset")

print("Path to dataset files:", path)

"""load dataset dari kaggle"""

books = pd.read_csv(path + "/Books Data with Category Language and Summary/Preprocessed_data.csv")
books.shape

books.head()

"""### Univariate analysis"""

books.info()

books.describe().round(2)

"""Didapat bahwa ada rating yang 0 atau artinya user tidak memberi rating"""

print("Jumlah missing values pada setiap kolom:", books.isnull().sum())

print("Jumlah duplikat:", books.duplicated().sum())

print("Jumlah judul buku : ", books['book_title'].nunique())
print("Jumlah User :" , books['user_id'].nunique())
print("Rating buku", books['rating'].unique())
print("Jumlah kategori :", books['Category'].nunique())
print("Jumlah bahasa :", books['Language'].nunique())
print("Bahasa :", books['Language'].unique())

"""*   Didapat banyak missing value pada beberapa kolom
*   Dapat dilihat juga buku memiliki beragam kode bahasa

## Data Preperation

### Mengambil buku dengan kode bahasa en
"""

#hanya ambil buku yang berbahasa inggris
books = books[books['Language'] == 'en']

"""### Drop buku dengan rating 0"""

#drop buku dengan rating 0
books = books[books['rating'] > 0]

books.info()

"""### Drop missing value"""

#Drop missing value
books = books.dropna()

"""### Ambil sample buku untuk modelling sebanyak 30000 baris"""

#Ambil sample 30000 baris agar tidak memperberat proses komputasi
books = books.sample(n=30000, random_state=42)

books.head()



"""## Persiapan data untuk content based Filtering"""

preperation = books.sort_values("isbn",ascending=True)
preperation = preperation.drop_duplicates(subset="isbn")
preperation = preperation.drop_duplicates(subset="book_title")
preperation = preperation.dropna()
preperation

"""mendrop buku dengan isbn yang sama dan buku dengan tittle yang sama agar tidak munvul berulang di rekomendasi"""



"""Mengambil hanya kolom yang akan digunakan untuk modelling Contentbased filtering"""

#Mengkonversi data dalam kolom menjadi list
book_id = preperation['isbn'].tolist()
book_title = preperation['book_title'].tolist()
category = preperation['Category'].tolist()

# Membuat dictionary untuk data
data = pd.DataFrame({
    'isbn': book_id,
    'book_title': book_title,
    'category': category
})

"""### Tfidf vectorizer"""

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer()

# Melakukan perhitungan idf pada data category
tfidf.fit(data['category'])
tfidf.get_feature_names_out()

tfidf_matrix = tfidf.fit_transform(data['category'])
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data.book_title
).sample(22, axis=1).sample(10, axis=0)



"""## Persiapan untuk Collaborative Filtering"""

df =books

"""###Encode user_id dan isbn agar menjadi berurutan

"""

user_ids = df['user_id'].unique().tolist()
print('list user_id: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

book_ids = df['isbn'].unique().tolist()

# Melakukan proses encoding isbn
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Melakukan proses encoding angka ke isbn
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

# Mapping user_idke dataframe user
df['user'] = df['user_id'].map(user_to_user_encoded)

# Mapping isbn ke dataframe buku
df['book'] = df['isbn'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_book = len(book_encoded_to_book)
print(num_book)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""### Split dataset"""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Content based Filtering"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap juful buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)



"""### Fungsi untuk memberikan top 10 rekomendasi buku"""

def book_recommendations(nama_buku, similarity_data=cosine_sim_df, items=data[['book_title', 'category']], k=10):
    """
    Rekomendasi buku berdasarkan kemiripan dataframe

    Parameter:
    ---
    nama_buku : tipe data string (str)
                Nama Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_buku agar judul buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_buku, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

book_recommendations('Accidental City: The Transformation of Toronto')

"""## Collaborative Filtering

Membangun model
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 128,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""Plot evaluasi"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""model sudah cukup konvergen jika diteruskan pun hasilnya akan sama

### Top 10 Rekomendasi untuk user acak
"""

book_df = books
df = df

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
book_read_by_user = df[df.user_id == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = book_df[~book_df['isbn'].isin(book_read_by_user.isbn.values)]['isbn']
book_not_read = list(
    set(book_not_read)
    .intersection(set(book_to_book_encoded.keys()))
)

book_not_read = [[book_to_book_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

# Predict ratings untuk buku yang belum dibaca pengguna
ratings = model.predict(user_book_array).flatten()

# Ambil 10 indeks dengan rating tertinggi (pastikan 10 buku unik)
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Kumpulkan ISBN rekomendasi, pastikan tidak ada duplikat
seen_isbns = set()
recommended_book_ids = []

for x in top_ratings_indices:
    book_id = book_encoded_to_book.get(book_not_read[x][0])
    # Tambahkan hanya ISBN yang belum ada di `seen_isbns`
    if book_id not in seen_isbns:
        seen_isbns.add(book_id)
        recommended_book_ids.append(book_id)
    # Hentikan loop jika sudah 10 ISBN unik
    if len(recommended_book_ids) == 10:
        break

# Tampilkan rekomendasi
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(by='rating', ascending=False)
    .head(5)
    .isbn.values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_title, ':', row.Category)

print('----' * 8)
print('Top 10 Book Recommendation')
print('----' * 8)

# Ambil data buku rekomendasi dan pastikan tidak ada duplikat ISBN
recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)].drop_duplicates(subset='isbn')

# Tampilkan hanya 10 buku pertama (jika ada duplikasi di data sumber)
for row in recommended_book.head(10).itertuples():
    print(row.book_title, ':', row.Category)

